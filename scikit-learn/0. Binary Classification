import re
import string
import pandas as pd
import numpy as np
from functools import lru_cache

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from spacy.lang.en import English

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

df_amazon = pd.read_excel(r'C:\Users\mayadi\Documents\Work\PWM\Code\New 11-2021\amazon_alexa.xlsx').head(500)
X         = df_amazon['verified_reviews'] 
ylabels   = df_amazon['feedback']


# configuration
punctuations = string.punctuation
nlp = spacy.load("en_core_web_sm")
stop_words = spacy.lang.en.stop_words.STOP_WORDS

@lru_cache(maxsize=10000)
def spacy_tokenizer(sentence):
    
    mytokens = []
    # Remove trailling and overflow white spaces
    sentence = re.sub("\s\s+" , " ", sentence.strip()) #takes too much time
    
    # Lemmatizing each token and converting each token into lowercase
    mytokens = [mytokens.append(word.lemma_) or word.lemma_ for word in nlp(sentence)] # it automatically lowercase the letters

    # Removing stop words and punctuation
    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

    return mytokens

### Test
#%%time
#sentences = sentences.tolist()
#df_tmp = list(map(lambda sentence : spacy_tokenizer(sentence), sentences))
#df_tmp[:3]



# # Install Spacy first 
# download https://github.com/explosion/spacy-models/releases/tag/de_core_news_sm-3.2.0
# the put it in C:\Users\mayadi\Anaconda3\Lib\site-packages
# extract the .gz then the .tar (replace all files when prompt)
# then open cmd fom the folder that has the setup.py file within the extracted folders and run the following: python setup.py install
# the dictionnairy will be downloaded, but when in the cmd it shows that it's processing dependancies you need to see which one is making a problem then interupt the execution on the terminal with ctrl+c before it arrives to that dependancy (the problem changes from one env to another).
# restart the notebook kernal, and it works like a charm =)

# import spacy
# from spacy.lang.de.examples import sentences 

# nlp = spacy.load("de_core_news_sm")
# doc = nlp(sentences[0])
# print(doc.text)
# for token in doc:
#     print(token.text, token.pos_, token.dep_)


import re, os
import string
import pandas as pd
import numpy as np
from functools import lru_cache
# accelerate
# https://stackoverflow.com/questions/62140599/countvectorizer-takes-too-long-to-fit-transform

import spacy
import sklearn
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate # to use sevrela validation metrics
from sklearn.model_selection import ShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier

from sklearn import svm
from sklearn.utils import shuffle
from sklearn import metrics


news = pd.read_excel(r'')

print('Number of empty cells: ', news['text'].isnull().sum())
news['text'].replace('  ', np.nan, inplace=True)
news = news.dropna(subset=['text'])

print('Number of empty cells: ', news['text'].isnull().sum())
print(news['relevant'].value_counts())
news.head()

news['relevant'] = news['relevant'].replace(['x', 'x '] , 1) # relevant
news['relevant'] = news['relevant'].replace(np.nan, 0)       # non relevant  

print(news['relevant'].value_counts())
news.head()

X         = news['text'].astype(str) 
ylabels   = news['relevant']
X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.25, shuffle=True, stratify=ylabels)

punctuations = string.punctuation
nlp = spacy.load("de_core_news_sm")
stop_words = spacy.lang.de.stop_words.STOP_WORDS

@lru_cache(maxsize=10000)
def spacy_tokenizer(sentence):
    
    mytokens = []
    
#     # configuration
#     mytokens = []
#     punctuations = string.punctuation
#     nlp = spacy.load("de_core_news_sm")
#     stop_words = spacy.lang.de.stop_words.STOP_WORDS
    
#     # Remove trailling and overflow white spaces
#     sentence = re.sub("\s\s+" , " ", sentence.strip()) #takes too much time
    
    # Lemmatizing each token and converting each token into lowercase
    mytokens = [mytokens.append(word.lemma_) or word.lemma_ for word in nlp(re.sub("\s\s+" , " ", sentence.strip()))] # it automatically lowercase the letters

    # Removing stop words and punctuation
    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

    return mytokens

### Test
#%%time
#sentences = sentences.tolist()
#df_tmp = list(map(lambda sentence : spacy_tokenizer(sentence), sentences))
#df_tmp[:3]

%%time

text_clf = Pipeline([('vect' , CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                     ('clf'  , MultinomialNB())])

model     = text_clf.fit(X_train, y_train)
predicted = model.predict(X_test)

print("Logistic Regression Accuracy : {:0.4f}".format(metrics.accuracy_score (y_test, predicted)))
print("Logistic Regression Precision: {:0.4f}".format(metrics.precision_score(y_test, predicted)))
print("Logistic Regression Recall   : {:0.4f}".format(metrics.recall_score   (y_test, predicted)))

%%time

X         = news['text'].astype(str) 
ylabels   = news['relevant']
X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.25, shuffle=True, stratify=ylabels)

text_clf   = Pipeline([ ('tfidf', TfidfVectorizer()),               
                        ('clf'  , SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42))])

parameters = {'tfidf__ngram_range': [(1, 1), (1, 2)],              
              'tfidf__use_idf'    : (True, False),
              'clf__alpha'        : (1e-2, 1e-3)}

gs_clf   = GridSearchCV(estimator=text_clf, param_grid=parameters, n_jobs=-1)
model_gs = gs_clf.fit(X_train, y_train)

print('{:0.4f}'.format(model_gs.best_score_))
print(model_gs.best_params_)

predicted = model.predict(X_test)
print("Accuracy : {:0.4f}".format(metrics.accuracy_score (y_test, predicted)))
print("Precision: {:0.4f}".format(metrics.precision_score(y_test, predicted)))
print("Recall   : {:0.4f}".format(metrics.recall_score   (y_test, predicted)))
